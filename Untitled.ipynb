{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f46159-7101-4b8f-a5e4-178878b5509e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 2018 ===\n",
      "- ccg        GPPS 2018 CCG data (weighted) (csv) PUBLIC.csv  [downloaded]\n",
      "- national   GPPS 2018 National data (weighted) (csv) PUBLIC.csv  [downloaded]\n",
      "- practice   GPPS 2018 Practice data (weighted) (csv) PUBLIC.csv  [downloaded]\n",
      "- variables  GPPS 2018 List of reporting variables (csv) PUBLIC.csv  [downloaded]\n",
      "\n",
      "=== 2019 ===\n",
      "- ccg        GPPS_2019_CCG_data_(weighted)_(csv)_PUBLIC.csv  [downloaded]\n",
      "- national   GPPS_2019_National_data_(weighted)_(csv)_PUBLIC.csv  [downloaded]\n",
      "- practice   GPPS_2019_Practice_data_(weighted)_(csv)_PUBLIC.csv  [downloaded]\n",
      "\n",
      "=== 2020 ===\n",
      "- ccg        GPPS_2020_CCG_data_(weighted)_(csv)_PUBLIC.csv  [downloaded]\n",
      "- national   GPPS_2020_National_data_(weighted)_(csv)_PUBLIC.csv  [downloaded]\n",
      "- practice   GPPS_2020_Practice_data_(weighted)_(csv)_PUBLIC.csv  [downloaded]\n",
      "\n",
      "=== 2021 ===\n",
      "- ccg        GPPS_2021_CCG_data_(weighted)_(csv)_PUBLIC.csv  [downloaded]\n",
      "- national   GPPS_2021_National_data_(weighted)_(csv)_PUBLIC.csv  [downloaded]\n",
      "- practice   GPPS_2021_Practice_data_(weighted)_(csv)_PUBLIC.csv  [downloaded]\n",
      "\n",
      "=== 2022 ===\n",
      "- ics        GPPS_2022_ICS_data_(weighted)_(csv)_PUBLIC.csv  [downloaded]\n",
      "- national   GPPS_2022_National_data_(weighted)_(csv)_PUBLIC.csv  [downloaded]\n",
      "- pcn        GPPS_2022_PCN_data_(weighted)_(csv)_PUBLIC.csv  [downloaded]\n",
      "- practice   GPPS_2022_Practice_data_(weighted)_(csv)_PUBLIC.csv  [downloaded]\n",
      "\n",
      "=== 2023 ===\n",
      "- ics        GPPS_2023_ICS_data_(weighted)_(csv)_PUBLIC.csv  [downloaded]\n",
      "- national   GPPS_2023_National_data_(weighted)_(csv)_PUBLIC.csv  [downloaded]\n",
      "- pcn        GPPS_2023_PCN_data_(weighted)_(csv)_PUBLIC.csv  [downloaded]\n",
      "- practice   GPPS_2023_Practice_data_(weighted)_(csv)_PUBLIC.csv  [downloaded]\n",
      "\n",
      "=== 2024 ===\n",
      "- ics        GPPS_2024_ICS_data_(weighted)_(csv)_PUBLIC.csv  [exists]\n",
      "- national   GPPS_2024_National_data_(weighted)_(csv)_PUBLIC.csv  [exists]\n",
      "- pcn        GPPS_2024_PCN_data_(weighted)_(csv)_PUBLIC.csv  [exists]\n",
      "- practice   GPPS_2024_Practice_data_(weighted)_(csv)_PUBLIC.csv  [exists]\n",
      "\n",
      "=== 2025 ===\n",
      "- ics        GPPS_2025_ICS_data_(weighted)_(csv)_PUBLIC.csv  [exists]\n",
      "- national   GPPS_2025_National_data_(weighted)_(csv)_PUBLIC.csv  [exists]\n",
      "- pcn        GPPS_2025_PCN_data_(weighted)_(csv)_PUBLIC.csv  [exists]\n",
      "- practice   GPPS_2025_Practice_data_(weighted)_(csv)_PUBLIC.csv  [exists]\n",
      "- region     GPPS_2025_Region_data_(weighted)_(csv)_PUBLIC.csv  [exists]\n",
      "\n",
      "=== SUMMARY ===\n",
      "downloaded=21, exists=9, missing=0\n",
      "Manifest: /Users/CajaLtd/Desktop/nhs patient survey/bronze/gpps/gpps_bronze_manifest.csv\n",
      "Bronze root: /Users/CajaLtd/Desktop/nhs patient survey/bronze/gpps\n"
     ]
    }
   ],
   "source": [
    "# GPPS bronze downloader — matches user-confirmed patterns (2018 archive/Weighted + 2019..2025)\n",
    "# --------------------------------------------------------------------------------------------\n",
    "# - Partitions: ./bronze/gpps/year=YYYY/level=<national|ccg|practice|ics|pcn|region|variables>/\n",
    "# - Files: original CSV filenames preserved\n",
    "# - Manifest: bronze/gpps/gpps_bronze_manifest.csv\n",
    "#\n",
    "# Requires: requests  (pip install requests)\n",
    "\n",
    "import os, csv, time, hashlib, urllib.parse\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import requests\n",
    "\n",
    "# =====================\n",
    "# Config\n",
    "# =====================\n",
    "OUT_ROOT = Path(\"./bronze/gpps\")\n",
    "START_YEAR = 2018\n",
    "END_YEAR   = 2025\n",
    "SLEEP_BETWEEN = 0.15\n",
    "TIMEOUT = 45\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "HOSTS = [\"https://gp-patient.co.uk\", \"https://www.gp-patient.co.uk\"]\n",
    "FILE_EP = [\"/FileDownload/Download\", \"/fileDownload/download\"]\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; GPPS-Scraper/1.0)\",\n",
    "    \"Referer\": \"https://www.gp-patient.co.uk/surveysandreports\",\n",
    "}\n",
    "\n",
    "# Which levels exist by year\n",
    "LEVELS_BY_YEAR = {\n",
    "    2018: {\"national\", \"ccg\", \"practice\", \"variables\"},   # variables = List of reporting variables (csv)\n",
    "    2019: {\"national\", \"ccg\", \"practice\"},\n",
    "    2020: {\"national\", \"ccg\", \"practice\"},\n",
    "    2021: {\"national\", \"ccg\", \"practice\"},\n",
    "    2022: {\"national\", \"ics\", \"pcn\", \"practice\"},\n",
    "    2023: {\"national\", \"ics\", \"pcn\", \"practice\"},\n",
    "    2024: {\"national\", \"ics\", \"pcn\", \"practice\"},\n",
    "    2025: {\"national\", \"region\", \"ics\", \"pcn\", \"practice\"},\n",
    "}\n",
    "\n",
    "LABEL = {\n",
    "    \"national\": \"National\",\n",
    "    \"ccg\":      \"CCG\",\n",
    "    \"practice\": \"Practice\",\n",
    "    \"ics\":      \"ICS\",\n",
    "    \"pcn\":      \"PCN\",\n",
    "    \"region\":   \"Region\",\n",
    "    \"variables\":\"List of reporting variables\"\n",
    "}\n",
    "\n",
    "# Optional “pin” URLs you provided (force-try these first)\n",
    "PINNED = {\n",
    "    # 2018\n",
    "    (\"2018\",\"national\"): \"archive/2018/Weighted/GPPS 2018 National data (weighted) (csv) PUBLIC.csv\",\n",
    "    (\"2018\",\"ccg\"):      \"archive/2018/Weighted/GPPS 2018 CCG data (weighted) (csv) PUBLIC.csv\",\n",
    "    (\"2018\",\"practice\"): \"archive/2018/Weighted/GPPS 2018 Practice data (weighted) (csv) PUBLIC.csv\",\n",
    "    (\"2018\",\"variables\"):\"archive/2018/GPPS 2018 List of reporting variables (csv) PUBLIC.csv\",\n",
    "    # 2019 national sample (others built from patterns below)\n",
    "    (\"2019\",\"national\"): \"2019/Weighted/GPPS_2019_National_data_(weighted)_(csv)_PUBLIC.csv\",\n",
    "}\n",
    "\n",
    "# =====================\n",
    "# Helpers\n",
    "# =====================\n",
    "def ensure_dir(p: Path): p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def sha256_of(path: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024*1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def underscored_filename(year: int, level: str) -> str:\n",
    "    return f\"GPPS_{year}_{LABEL[level]}_data_(weighted)_(csv)_PUBLIC.csv\"\n",
    "\n",
    "def spaced_2018(level: str) -> str:\n",
    "    # 2018 uses spaces\n",
    "    if level == \"variables\":\n",
    "        return \"GPPS 2018 List of reporting variables (csv) PUBLIC.csv\"\n",
    "    return f\"GPPS 2018 {LABEL[level]} data (weighted) (csv) PUBLIC.csv\"\n",
    "\n",
    "def join_url(*parts: str) -> str:\n",
    "    return \"/\".join(s.strip(\"/\") for s in parts)\n",
    "\n",
    "def build_candidates(year: int, level: str) -> List[Tuple[str,str]]:\n",
    "    \"\"\"\n",
    "    Returns ordered candidates as ('fileRedirect', relative-path) or ('direct','Downloads/...').\n",
    "    We honor your confirmed links first (PINNED).\n",
    "    \"\"\"\n",
    "    cands: List[Tuple[str,str]] = []\n",
    "\n",
    "    # 1) Pinned (from your message)\n",
    "    key = (str(year), level)\n",
    "    if key in PINNED:\n",
    "        cands.append((\"fileRedirect\", PINNED[key]))\n",
    "\n",
    "    # 2) Year-specific rules\n",
    "    if year == 2018:\n",
    "        fn = spaced_2018(level)\n",
    "        if level == \"variables\":\n",
    "            # not in Weighted\n",
    "            cands.append((\"fileRedirect\", join_url(\"archive\",\"2018\", fn)))\n",
    "            cands.append((\"direct\", join_url(\"Downloads\",\"archive\",\"2018\", fn)))\n",
    "            cands.append((\"direct\", join_url(\"downloads\",\"archive\",\"2018\", fn)))\n",
    "        else:\n",
    "            cands.append((\"fileRedirect\", join_url(\"archive\",\"2018\",\"Weighted\", fn)))\n",
    "            cands.append((\"direct\", join_url(\"Downloads\",\"archive\",\"2018\", \"Weighted\", fn)))\n",
    "            cands.append((\"direct\", join_url(\"downloads\",\"archive\",\"2018\", \"Weighted\", fn)))\n",
    "        return dedupe(cands)\n",
    "\n",
    "    if 2019 <= year <= 2021:\n",
    "        fn = underscored_filename(year, level)\n",
    "        cands.append((\"fileRedirect\", join_url(str(year), \"Weighted\", fn)))\n",
    "        cands.append((\"direct\", join_url(\"Downloads\", str(year), fn)))\n",
    "        cands.append((\"direct\", join_url(\"downloads\", str(year), fn)))\n",
    "        return dedupe(cands)\n",
    "\n",
    "    if 2022 <= year <= 2023:\n",
    "        fn = underscored_filename(year, level)\n",
    "        cands.append((\"fileRedirect\", join_url(str(year), \"Weighted\", fn)))\n",
    "        cands.append((\"direct\", join_url(\"Downloads\", str(year), fn)))\n",
    "        cands.append((\"direct\", join_url(\"downloads\", str(year), fn)))\n",
    "        return dedupe(cands)\n",
    "\n",
    "    if year == 2024:\n",
    "        fn = underscored_filename(year, level)\n",
    "        cands.append((\"fileRedirect\", join_url(str(year), fn)))  # seen live\n",
    "        cands.append((\"fileRedirect\", join_url(str(year), \"Weighted\", fn)))  # fallback\n",
    "        cands.append((\"direct\", join_url(\"Downloads\", str(year), fn)))\n",
    "        cands.append((\"direct\", join_url(\"downloads\", str(year), fn)))\n",
    "        return dedupe(cands)\n",
    "\n",
    "    if year >= 2025:\n",
    "        fn = underscored_filename(year, level)\n",
    "        cands.append((\"fileRedirect\", join_url(str(year), \"survey-results\", f\"{level}-results\", f\"{level}-data-csv\", fn)))\n",
    "        cands.append((\"direct\", join_url(\"Downloads\", str(year), fn)))\n",
    "        cands.append((\"direct\", join_url(\"downloads\", str(year), fn)))\n",
    "        return dedupe(cands)\n",
    "\n",
    "    return dedupe(cands)\n",
    "\n",
    "def dedupe(items: List[Tuple[str,str]]) -> List[Tuple[str,str]]:\n",
    "    seen = set(); out = []\n",
    "    for t in items:\n",
    "        if t not in seen:\n",
    "            out.append(t); seen.add(t)\n",
    "    return out\n",
    "\n",
    "def is_csvish(resp: requests.Response) -> bool:\n",
    "    if resp is None: return False\n",
    "    if resp.status_code != 200: return False\n",
    "    ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
    "    if \"text/csv\" in ctype or \"application/octet-stream\" in ctype or \"application/vnd.ms-excel\" in ctype:\n",
    "        return True\n",
    "    cdisp = (resp.headers.get(\"Content-Disposition\") or \"\").lower()\n",
    "    return \".csv\" in cdisp\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update(HEADERS)\n",
    "adapter = requests.adapters.HTTPAdapter(pool_connections=8, pool_maxsize=8, max_retries=2)\n",
    "session.mount(\"https://\", adapter); session.mount(\"http://\", adapter)\n",
    "\n",
    "def probe_candidate(kind: str, relpath: str) -> Optional[str]:\n",
    "    # Return working URL or None\n",
    "    if kind == \"fileRedirect\":\n",
    "        q = urllib.parse.quote(relpath, safe=\"/() _-\")\n",
    "        for host in HOSTS:\n",
    "            for ep in FILE_EP:\n",
    "                url = f\"{host}{ep}?fileRedirect={q}\"\n",
    "                try:\n",
    "                    r = session.get(url, timeout=TIMEOUT, stream=True, allow_redirects=True)\n",
    "                except requests.RequestException:\n",
    "                    r = None\n",
    "                if r:\n",
    "                    good = is_csvish(r)\n",
    "                    r.close()\n",
    "                    if good:\n",
    "                        return url\n",
    "        return None\n",
    "    else:  # direct under /Downloads or /downloads\n",
    "        rel = relpath.lstrip(\"/\")\n",
    "        for host in HOSTS:\n",
    "            url = join_url(host, rel)\n",
    "            try:\n",
    "                r = session.get(url, timeout=TIMEOUT, stream=True, allow_redirects=True)\n",
    "            except requests.RequestException:\n",
    "                r = None\n",
    "            if r:\n",
    "                good = is_csvish(r)\n",
    "                r.close()\n",
    "                if good:\n",
    "                    return url\n",
    "        return None\n",
    "\n",
    "def download(url: str, dest: Path) -> bool:\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    for attempt in range(1, MAX_RETRIES+1):\n",
    "        try:\n",
    "            with session.get(url, timeout=TIMEOUT, stream=True) as r:\n",
    "                if r.status_code != 200:\n",
    "                    raise RuntimeError(f\"HTTP {r.status_code}\")\n",
    "                tmp = dest.with_suffix(dest.suffix + \".part\")\n",
    "                with tmp.open(\"wb\") as f:\n",
    "                    for chunk in r.iter_content(1024*256):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                tmp.replace(dest)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            if attempt == MAX_RETRIES:\n",
    "                print(f\"[ERROR] {url} -> {e}\")\n",
    "                return False\n",
    "            time.sleep(1.0)\n",
    "\n",
    "def out_dir(year: int, level: str) -> Path:\n",
    "    return OUT_ROOT / f\"year={year}\" / f\"level={level}\"\n",
    "\n",
    "def filename_from_rel(rel: str) -> str:\n",
    "    return Path(urllib.parse.unquote(rel)).name\n",
    "\n",
    "# =====================\n",
    "# Run\n",
    "# =====================\n",
    "ensure_dir(OUT_ROOT)\n",
    "manifest_rows: List[Dict[str,str]] = []\n",
    "downloaded = exists = missing = 0\n",
    "\n",
    "for year in range(START_YEAR, END_YEAR+1):\n",
    "    levels = LEVELS_BY_YEAR.get(year, set())\n",
    "    if not levels: \n",
    "        continue\n",
    "    print(f\"\\n=== {year} ===\")\n",
    "    for level in sorted(levels):\n",
    "        # Build candidates (your pinned first)\n",
    "        cands = build_candidates(year, level)\n",
    "\n",
    "        # Choose filename from first candidate\n",
    "        if cands:\n",
    "            first_rel = cands[0][1]\n",
    "            fname = filename_from_rel(first_rel)\n",
    "        else:\n",
    "            # Fallback filename pattern\n",
    "            fname = spaced_2018(level) if year == 2018 else underscored_filename(year, level)\n",
    "\n",
    "        dest = out_dir(year, level) / fname\n",
    "\n",
    "        if dest.exists() and dest.stat().st_size > 0:\n",
    "            print(f\"- {level:<10} {fname}  [exists]\")\n",
    "            exists += 1\n",
    "            manifest_rows.append({\n",
    "                \"year\": str(year), \"level\": level, \"status\": \"exists\",\n",
    "                \"url\": \"\", \"local_path\": str(dest.resolve()),\n",
    "                \"bytes\": str(dest.stat().st_size), \"sha256\": sha256_of(dest)\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Probe candidates in order\n",
    "        final_url = None\n",
    "        for kind, rel in cands:\n",
    "            url = probe_candidate(kind, rel)\n",
    "            if url:\n",
    "                final_url = url\n",
    "                break\n",
    "\n",
    "        if not final_url:\n",
    "            print(f\"- {level:<10} {fname}  [missing]\")\n",
    "            missing += 1\n",
    "            manifest_rows.append({\n",
    "                \"year\": str(year), \"level\": level, \"status\": \"missing\",\n",
    "                \"url\": \"\", \"local_path\": str(dest.resolve()),\n",
    "                \"bytes\": \"0\", \"sha256\": \"\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        ok = download(final_url, dest)\n",
    "        if ok:\n",
    "            print(f\"- {level:<10} {fname}  [downloaded]\")\n",
    "            downloaded += 1\n",
    "            manifest_rows.append({\n",
    "                \"year\": str(year), \"level\": level, \"status\": \"downloaded\",\n",
    "                \"url\": final_url, \"local_path\": str(dest.resolve()),\n",
    "                \"bytes\": str(dest.stat().st_size), \"sha256\": sha256_of(dest)\n",
    "            })\n",
    "        else:\n",
    "            print(f\"- {level:<10} {fname}  [error]\")\n",
    "            missing += 1\n",
    "            manifest_rows.append({\n",
    "                \"year\": str(year), \"level\": level, \"status\": \"error\",\n",
    "                \"url\": final_url, \"local_path\": str(dest.resolve()),\n",
    "                \"bytes\": \"0\", \"sha256\": \"\"\n",
    "            })\n",
    "\n",
    "        time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "# Write manifest\n",
    "manifest_path = OUT_ROOT / \"gpps_bronze_manifest.csv\"\n",
    "with manifest_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"year\",\"level\",\"status\",\"url\",\"local_path\",\"bytes\",\"sha256\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(manifest_rows)\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"downloaded={downloaded}, exists={exists}, missing={missing}\")\n",
    "print(f\"Manifest: {manifest_path.resolve()}\")\n",
    "print(f\"Bronze root: {OUT_ROOT.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baeca345-501d-47c9-9342-ce8527f31b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[READ] bronze/gpps/year=2024/level=variables/GPPS_2024_List_of_reporting_variables_(csv)_PUBLIC.xlsx (sheet=GPPS_2024_list_of_reporting_var)\n",
      "[WRITE] bronze/gpps/year=2024/level=variables/GPPS_2024_List_of_reporting_variables_(csv)_PUBLIC.csv\n",
      "[READ] bronze/gpps/year=2025/level=variables/GPPS_2025_List_of_reporting_variables_PUBLIC.xlsx (sheet=GPPS_2025_list_of_reporting_var)\n",
      "[WRITE] bronze/gpps/year=2025/level=variables/GPPS_2025_List_of_reporting_variables_PUBLIC.csv\n",
      "[DONE] Converted GPPS variables XLSX → CSV\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Root of your GPPS bronze, adjust if needed\n",
    "BRONZE_ROOT = Path(\"./bronze/gpps\")\n",
    "\n",
    "# Years where variables are in Excel format\n",
    "YEARS = [2024, 2025]\n",
    "\n",
    "# Sheet name pattern in the Excel files\n",
    "# e.g. GPPS_2024_list_of_reporting_var, GPPS_2025_list_of_reporting_var\n",
    "SHEET_TEMPLATE = \"GPPS_{year}_list_of_reporting_var\"\n",
    "\n",
    "def find_xlsx_files(base_dir: Path):\n",
    "    \"\"\"Yield all .xlsx files under base_dir.\"\"\"\n",
    "    if not base_dir.exists():\n",
    "        return\n",
    "    for p in base_dir.glob(\"*.xlsx\"):\n",
    "        yield p\n",
    "\n",
    "def convert_xlsx_to_csv(xlsx_path: Path, year: int):\n",
    "    \"\"\"\n",
    "    Read GPPS variables Excel and write a CSV in the same folder.\n",
    "    Keeps only the main 'list_of_reporting_var' sheet.\n",
    "    \"\"\"\n",
    "    sheet_name = SHEET_TEMPLATE.format(year=year)\n",
    "    print(f\"[READ] {xlsx_path} (sheet={sheet_name})\")\n",
    "\n",
    "    # Read just the variables sheet\n",
    "    df = pd.read_excel(xlsx_path, sheet_name=sheet_name)\n",
    "\n",
    "    # Optional: trim obvious junk columns if present\n",
    "    # (drop completely empty columns)\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "    csv_name = xlsx_path.with_suffix(\".csv\").name\n",
    "    csv_path = xlsx_path.with_suffix(\".csv\")\n",
    "\n",
    "    print(f\"[WRITE] {csv_path}\")\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # If you want to remove the original Excel to avoid confusion, uncomment:\n",
    "    # print(f\"[DELETE] {xlsx_path}\")\n",
    "    # xlsx_path.unlink()\n",
    "\n",
    "def main():\n",
    "    for year in YEARS:\n",
    "        var_dir = BRONZE_ROOT / f\"year={year}\" / \"level=variables\"\n",
    "        if not var_dir.exists():\n",
    "            print(f\"[SKIP] No variables folder for year={year}: {var_dir}\")\n",
    "            continue\n",
    "\n",
    "        xlsx_files = list(find_xlsx_files(var_dir))\n",
    "        if not xlsx_files:\n",
    "            print(f\"[SKIP] No .xlsx files found in {var_dir}\")\n",
    "            continue\n",
    "\n",
    "        for xlsx in xlsx_files:\n",
    "            convert_xlsx_to_csv(xlsx, year)\n",
    "\n",
    "    print(\"[DONE] Converted GPPS variables XLSX → CSV\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06b0b9-5d92-41c8-bd83-0673a182a549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
